{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11810064,"sourceType":"datasetVersion","datasetId":7417358},{"sourceId":11811236,"sourceType":"datasetVersion","datasetId":7418211},{"sourceId":11835960,"sourceType":"datasetVersion","datasetId":7436034},{"sourceId":11836367,"sourceType":"datasetVersion","datasetId":7436321}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyro-ppl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:23:59.641862Z","iopub.execute_input":"2025-05-17T07:23:59.642118Z","iopub.status.idle":"2025-05-17T07:25:09.919406Z","shell.execute_reply.started":"2025-05-17T07:23:59.642098Z","shell.execute_reply":"2025-05-17T07:25:09.918748Z"}},"outputs":[{"name":"stdout","text":"Collecting pyro-ppl\n  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (3.4.0)\nCollecting pyro-api>=0.1.1 (from pyro-ppl)\n  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (2.6.0+cu124)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pyro-ppl) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.7->pyro-ppl) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->pyro-ppl)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->pyro-ppl)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->pyro-ppl)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->pyro-ppl)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->pyro-ppl)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->pyro-ppl)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->pyro-ppl)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->pyro-ppl) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->pyro-ppl) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.7->pyro-ppl) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.7->pyro-ppl) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.7->pyro-ppl) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.7->pyro-ppl) (2024.2.0)\nDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyro-api, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pyro-ppl\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyro-api-0.1.2 pyro-ppl-1.9.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.infer.autoguide import AutoDelta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:25:09.920892Z","iopub.execute_input":"2025-05-17T07:25:09.921154Z","iopub.status.idle":"2025-05-17T07:25:13.660178Z","shell.execute_reply.started":"2025-05-17T07:25:09.921130Z","shell.execute_reply":"2025-05-17T07:25:13.659600Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Carica i dati\nX_array = np.load(\"/kaggle/input/bow-xarray/BoW_X_Array.npz\")[\"arr_0\"]\nX_tensor = torch.tensor(X_array, dtype=torch.float)\nprint(\"XArray added\")\n\n# Opzionale: usa GPU se disponibile\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\nX_tensor = X_tensor.to(device)\n\nnum_docs, vocab_size = X_tensor.shape\nK_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90]\nresults = []\n\nfor K in K_values:\n    print(f\"\\n--- Training LDA with K = {K} ---\")\n\n    def lda_model(data):\n        with pyro.plate(\"topics\", K):\n            topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(vocab_size).to(device)))\n        with pyro.plate(\"documents\", num_docs):\n            doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(K).to(device)))\n            word_dists = torch.matmul(doc_topics, topic_words)\n            logits = torch.matmul(doc_topics, topic_words).log()\n            pyro.sample(\"doc_words\", dist.Multinomial(total_count=100, logits=logits), obs=data)\n\n    pyro.clear_param_store()\n    guide = AutoDelta(lda_model)\n    svi = SVI(lda_model, guide, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n\n    for step in range(500):\n        loss = svi.step(X_tensor)\n\n    posterior = guide()\n    doc_topics = posterior[\"doc_topics\"]\n    topic_usage = doc_topics.sum(dim=0).detach().cpu().numpy()\n\n    # Statistiche extra\n    loss_per_doc = loss / num_docs\n    entropy = -(doc_topics * doc_topics.log()).sum(dim=1).mean().item()\n    avg_active_per_doc = (doc_topics > 0.05).sum(dim=1).float().mean().item()\n    num_active_topics = (topic_usage > 5.0).sum()\n\n    results.append({\n        \"K\": K,\n        \"Final Loss\": float(loss),\n        \"Loss per Doc\": float(loss_per_doc),\n        \"Entropy\": float(entropy),\n        \"Avg Active Topics/Doc\": float(avg_active_per_doc),\n        \"Active Topics (global)\": int(num_active_topics)\n    })\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"results_k_selection.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:25:13.660879Z","iopub.execute_input":"2025-05-17T07:25:13.661231Z","iopub.status.idle":"2025-05-17T07:29:39.686357Z","shell.execute_reply.started":"2025-05-17T07:25:13.661213Z","shell.execute_reply":"2025-05-17T07:29:39.685798Z"}},"outputs":[{"name":"stdout","text":"XArray added\ncuda\n\n--- Training LDA with K = 5 ---\n\n--- Training LDA with K = 10 ---\n\n--- Training LDA with K = 15 ---\n\n--- Training LDA with K = 20 ---\n\n--- Training LDA with K = 25 ---\n\n--- Training LDA with K = 30 ---\n\n--- Training LDA with K = 35 ---\n\n--- Training LDA with K = 40 ---\n\n--- Training LDA with K = 45 ---\n\n--- Training LDA with K = 50 ---\n\n--- Training LDA with K = 55 ---\n\n--- Training LDA with K = 60 ---\n\n--- Training LDA with K = 65 ---\n\n--- Training LDA with K = 70 ---\n\n--- Training LDA with K = 75 ---\n\n--- Training LDA with K = 80 ---\n\n--- Training LDA with K = 85 ---\n\n--- Training LDA with K = 90 ---\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:29:39.687752Z","iopub.execute_input":"2025-05-17T07:29:39.688030Z","iopub.status.idle":"2025-05-17T07:29:39.710409Z","shell.execute_reply.started":"2025-05-17T07:29:39.688010Z","shell.execute_reply":"2025-05-17T07:29:39.709663Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     K    Final Loss  Loss per Doc   Entropy  Avg Active Topics/Doc  \\\n0    5 -4.243077e+04     -7.614998  0.377198               1.628141   \n1   10 -3.917967e+05    -70.315278  0.532416               1.924803   \n2   15 -7.485161e+05   -134.335267  0.613301               2.062096   \n3   20 -1.116392e+06   -200.357555  0.703568               2.164393   \n4   25 -1.486718e+06   -266.819421  0.804096               2.395908   \n5   30 -1.860040e+06   -333.819168  0.820487               2.374013   \n6   35 -2.237448e+06   -401.552029  0.874397               2.456389   \n7   40 -2.618543e+06   -469.946670  0.962098               2.622039   \n8   45 -3.000189e+06   -538.440243  1.102405               3.023690   \n9   50 -3.386729e+06   -607.812133  0.990354               2.613963   \n10  55 -3.774782e+06   -677.455447  1.321905               3.497846   \n11  60 -4.170445e+06   -748.464583  1.326665               3.421572   \n12  65 -4.559706e+06   -818.324808  1.347632               3.572864   \n13  70 -4.959357e+06   -890.049693  1.254022               3.028894   \n14  75 -5.360137e+06   -961.977205  1.300322               3.132627   \n15  80 -5.757895e+06  -1033.362314  1.341705               3.198672   \n16  85 -6.163201e+06  -1106.102129  1.479553               3.201364   \n17  90 -6.565427e+06  -1178.289175  1.293504               2.849067   \n\n    Active Topics (global)  \n0                        5  \n1                       10  \n2                       15  \n3                       20  \n4                       25  \n5                       30  \n6                       35  \n7                       40  \n8                       45  \n9                       50  \n10                      55  \n11                      60  \n12                      65  \n13                      70  \n14                      75  \n15                      80  \n16                      85  \n17                      76  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>K</th>\n      <th>Final Loss</th>\n      <th>Loss per Doc</th>\n      <th>Entropy</th>\n      <th>Avg Active Topics/Doc</th>\n      <th>Active Topics (global)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>-4.243077e+04</td>\n      <td>-7.614998</td>\n      <td>0.377198</td>\n      <td>1.628141</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>-3.917967e+05</td>\n      <td>-70.315278</td>\n      <td>0.532416</td>\n      <td>1.924803</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15</td>\n      <td>-7.485161e+05</td>\n      <td>-134.335267</td>\n      <td>0.613301</td>\n      <td>2.062096</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>-1.116392e+06</td>\n      <td>-200.357555</td>\n      <td>0.703568</td>\n      <td>2.164393</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25</td>\n      <td>-1.486718e+06</td>\n      <td>-266.819421</td>\n      <td>0.804096</td>\n      <td>2.395908</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>30</td>\n      <td>-1.860040e+06</td>\n      <td>-333.819168</td>\n      <td>0.820487</td>\n      <td>2.374013</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>35</td>\n      <td>-2.237448e+06</td>\n      <td>-401.552029</td>\n      <td>0.874397</td>\n      <td>2.456389</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>40</td>\n      <td>-2.618543e+06</td>\n      <td>-469.946670</td>\n      <td>0.962098</td>\n      <td>2.622039</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>45</td>\n      <td>-3.000189e+06</td>\n      <td>-538.440243</td>\n      <td>1.102405</td>\n      <td>3.023690</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>50</td>\n      <td>-3.386729e+06</td>\n      <td>-607.812133</td>\n      <td>0.990354</td>\n      <td>2.613963</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>55</td>\n      <td>-3.774782e+06</td>\n      <td>-677.455447</td>\n      <td>1.321905</td>\n      <td>3.497846</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>60</td>\n      <td>-4.170445e+06</td>\n      <td>-748.464583</td>\n      <td>1.326665</td>\n      <td>3.421572</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>65</td>\n      <td>-4.559706e+06</td>\n      <td>-818.324808</td>\n      <td>1.347632</td>\n      <td>3.572864</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>70</td>\n      <td>-4.959357e+06</td>\n      <td>-890.049693</td>\n      <td>1.254022</td>\n      <td>3.028894</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>75</td>\n      <td>-5.360137e+06</td>\n      <td>-961.977205</td>\n      <td>1.300322</td>\n      <td>3.132627</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>80</td>\n      <td>-5.757895e+06</td>\n      <td>-1033.362314</td>\n      <td>1.341705</td>\n      <td>3.198672</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>85</td>\n      <td>-6.163201e+06</td>\n      <td>-1106.102129</td>\n      <td>1.479553</td>\n      <td>3.201364</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>90</td>\n      <td>-6.565427e+06</td>\n      <td>-1178.289175</td>\n      <td>1.293504</td>\n      <td>2.849067</td>\n      <td>76</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\nfrom pyro.infer.autoguide import AutoDelta\nfrom sklearn.model_selection import train_test_split\n\n# === PARAMETRI ===\nK = 60\nnum_steps_train = 1000\nnum_steps_test_svi = 500\nnum_samples_mcmc = 800\nwarmup_mcmc = 300\ndirichlet_alpha = 0.1\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:29:39.711157Z","iopub.execute_input":"2025-05-17T07:29:39.711444Z","iopub.status.idle":"2025-05-17T07:29:40.241648Z","shell.execute_reply.started":"2025-05-17T07:29:39.711401Z","shell.execute_reply":"2025-05-17T07:29:40.241102Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndf_sms = pd.read_csv(\"/kaggle/input/sms-cleaned/lda_df.csv\")\n\n# Convert stringified lists to actual lists\ndf_sms[\"tokens\"] = df_sms[\"tokens\"].apply(ast.literal_eval)\n\n# Join tokens into space-separated strings\ndf_sms[\"joined_tokens\"] = df_sms[\"tokens\"].apply(lambda tokens: \" \".join(tokens))\n\n# Create BoW matrix\nvectorizer = CountVectorizer()\nX_array = vectorizer.fit_transform(df_sms[\"joined_tokens\"]).toarray()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:29:40.242350Z","iopub.execute_input":"2025-05-17T07:29:40.242717Z","iopub.status.idle":"2025-05-17T07:29:40.548021Z","shell.execute_reply.started":"2025-05-17T07:29:40.242670Z","shell.execute_reply":"2025-05-17T07:29:40.547424Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\nfrom pyro.infer.autoguide import AutoDelta\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport ast\n\n# === PARAMETRI ===\nK = 60\nnum_steps_train = 1000\nnum_steps_test_svi = 500\nnum_samples_mcmc = 800\nwarmup_mcmc = 300\ndirichlet_alpha = 0.1\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# === CARICAMENTO E PREPARAZIONE DATI ===\ndf_sms = pd.read_csv(\"/kaggle/input/sms-cleaned/lda_df.csv\")  # includes 'label', 'message', 'tokens'\ndf_sms[\"tokens\"] = df_sms[\"tokens\"].apply(ast.literal_eval)\ndf_sms[\"joined_tokens\"] = df_sms[\"tokens\"].apply(lambda tokens: \" \".join(tokens))\n\nvectorizer = CountVectorizer()\nX_array = vectorizer.fit_transform(df_sms[\"joined_tokens\"]).toarray()\n\n# === TRAIN/TEST SPLIT ===\nX_train_np, X_test_np, y_train, y_test = train_test_split(\n    X_array,\n    df_sms[\"label\"].values,\n    test_size=0.2,\n    stratify=df_sms[\"label\"],\n    random_state=42\n)\n\n# Save labels for classification later\npd.DataFrame({\"label\": y_train}).to_csv(\"sms_train.csv\", index=False)\npd.DataFrame({\"label\": y_test}).to_csv(\"sms_test.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:29:40.548638Z","iopub.execute_input":"2025-05-17T07:29:40.548888Z","iopub.status.idle":"2025-05-17T07:29:40.990470Z","shell.execute_reply.started":"2025-05-17T07:29:40.548871Z","shell.execute_reply":"2025-05-17T07:29:40.989932Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Convert to torch tensors\nX_train = torch.tensor(X_train_np, dtype=torch.float).to(device)\nX_test = torch.tensor(X_test_np, dtype=torch.float).to(device)\nnum_train, vocab_size = X_train.shape\nnum_test = X_test.shape[0]\n\n# === MODELLO LDA PER TRAINING ===\ndef lda_model(data, num_docs):\n    with pyro.plate(\"topics\", K):\n        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(vocab_size).to(device)))\n    with pyro.plate(\"documents\", num_docs):\n        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(dirichlet_alpha * torch.ones(K).to(device)))\n        logits = torch.matmul(doc_topics, topic_words).log()\n        pyro.sample(\"doc_words\", dist.Multinomial(total_count=100, logits=logits), obs=data)\n\n# === TRAIN LDA SU TRAIN SET CON SVI ===\npyro.clear_param_store()\nguide = AutoDelta(lambda data: lda_model(data, num_train))\nsvi = SVI(lambda data: lda_model(data, num_train), guide, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n\nprint(f\"\\n🚀 Training LDA on train set with SVI...\")\nfor step in range(num_steps_train):\n    loss = svi.step(X_train)\n    if step % 100 == 0:\n        print(f\"[step {step}] loss = {loss:.2f}\")\n\nposterior = guide(X_train)\ntopic_words = posterior[\"topic_words\"].detach()\ndoc_topics_train = posterior[\"doc_topics\"].detach().cpu().numpy()\n\npd.DataFrame(doc_topics_train, columns=[f\"topic_{i}\" for i in range(K)]).to_csv(\"doc_topics_train_K60.csv\", index=False)\nprint(\"DS train saved!\")\n\n# === INFERENZA SVI SUL TEST SET ===\ndef lda_inference_svi_model(data, topic_words_fixed, num_docs):\n    with pyro.plate(\"documents\", num_docs):\n        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(dirichlet_alpha * torch.ones(K).to(device)))\n        logits = torch.matmul(doc_topics, topic_words_fixed).log()\n        pyro.sample(\"doc_words\", dist.Multinomial(total_count=100, logits=logits), obs=data)\n\nguide_test_svi = AutoDelta(lambda data: lda_inference_svi_model(data, topic_words, num_test))\nsvi_test = SVI(lambda data: lda_inference_svi_model(data, topic_words, num_test),\n               guide_test_svi, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n\npyro.clear_param_store()\nprint(\"\\n⚡ Running SVI inference on test set...\")\nfor step in range(num_steps_test_svi):\n    svi_test.step(X_test)\n    if step % 100 == 0:\n        print(f\"[step {step}]\")\n\nposterior_test_svi = guide_test_svi(X_test)\ndoc_topics_test_svi = posterior_test_svi[\"doc_topics\"].detach().cpu().numpy()\n\npd.DataFrame(doc_topics_test_svi, columns=[f\"topic_{i}\" for i in range(K)]).to_csv(\"doc_topics_test_K60_svi.csv\", index=False)\n\n# === INFERENZA MCMC SUL TEST SET ===\ndef lda_inference_mcmc_model(data, topic_words_fixed, num_docs):\n    with pyro.plate(\"documents\", num_docs):\n        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(dirichlet_alpha * torch.ones(K).to(device)))\n        logits = torch.matmul(doc_topics, topic_words_fixed).log()\n        pyro.sample(\"doc_words\", dist.Multinomial(total_count=100, logits=logits), obs=data)\n\nmcmc_model = lambda data: lda_inference_mcmc_model(data, topic_words, num_test)\n\npyro.clear_param_store()\nnuts_kernel = NUTS(mcmc_model)\nmcmc = MCMC(nuts_kernel, num_samples=num_samples_mcmc, warmup_steps=warmup_mcmc, num_chains=1)\n\nprint(\"\\n🧊 Running MCMC inference on test set...\")\nmcmc.run(X_test)\n\nmcmc_samples = mcmc.get_samples()\ndoc_topics_test_mcmc = mcmc_samples[\"doc_topics\"].detach().cpu().numpy()\n\ndoc_topics_mcmc_mean = doc_topics_test_mcmc.mean(axis=0)\ndoc_topics_mcmc_std = doc_topics_test_mcmc.std(axis=0)\n\npd.DataFrame(doc_topics_mcmc_mean, columns=[f\"topic_{i}\" for i in range(K)]).to_csv(\"doc_topics_test_K60_mcmc_mean.csv\", index=False)\npd.DataFrame(doc_topics_mcmc_std, columns=[f\"topic_{i}\" for i in range(K)]).to_csv(\"doc_topics_test_K60_mcmc_std.csv\", index=False)\n\nprint(\"\\u2705 All results saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:29:40.991213Z","iopub.execute_input":"2025-05-17T07:29:40.991431Z"}},"outputs":[{"name":"stdout","text":"\n🚀 Training LDA on train set with SVI...\n[step 0] loss = -3478207.34\n[step 100] loss = -3590968.77\n[step 200] loss = -3937175.31\n[step 300] loss = -4266887.02\n[step 400] loss = -4660791.03\n[step 500] loss = -5097602.12\n[step 600] loss = -5623749.00\n[step 700] loss = -6222733.75\n[step 800] loss = -6854969.20\n[step 900] loss = -7495493.97\nDS train saved!\n\n⚡ Running SVI inference on test set...\n[step 0]\n[step 100]\n[step 200]\n[step 300]\n[step 400]\n\n🧊 Running MCMC inference on test set...\n","output_type":"stream"},{"name":"stderr","text":"Sample:  75%|███████▍  | 821/1100 [1:23:24,  5.16s/it, step size=9.55e-05, acc. prob=0.787]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Confronto tra SVI e MCMC","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\nsvi = pd.read_csv(\"/kaggle/working/doc_topics_test_K60_svi.csv\").iloc[0]\nmcmc_mean = pd.read_csv(\"/kaggle/working/doc_topics_test_K60_mcmc_mean.csv\").iloc[0]\nmcmc_std = pd.read_csv(\"/kaggle/working/doc_topics_test_K60_mcmc_std.csv\").iloc[0]\n\nplt.figure(figsize=(12, 4))\nplt.plot(svi, label=\"SVI\")\nplt.plot(mcmc_mean, label=\"MCMC Mean\", linestyle=\"--\")\nplt.fill_between(range(60), mcmc_mean - mcmc_std, mcmc_mean + mcmc_std, alpha=0.3, label=\"MCMC ± std\")\nplt.title(\"Distribuzione di topic per un documento di test\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Carica i file\ndoc_svi = pd.read_csv(\"/kaggle/working/doc_topics_test_K60_svi.csv\")\ndoc_mcmc_mean = pd.read_csv(\"/kaggle/working/doc_topics_test_K60_mcmc_mean.csv\")\ndoc_mcmc_std = pd.read_csv(\"/kaggle/working/doc_topics_test_K60_mcmc_std.csv\")\n\n# Prendiamo i primi 10 documenti\nN = 10\nsvi_batch = doc_svi.iloc[:N].values\nmcmc_mean_batch = doc_mcmc_mean.iloc[:N].values\nmcmc_std_batch = doc_mcmc_std.iloc[:N].values\n\n# === METRICHE ===\ndef entropy(p):\n    p = np.clip(p, 1e-12, 1.0)  # evita log(0)\n    return -np.sum(p * np.log(p), axis=1)\n\ndef active_topics(p, threshold=0.05):\n    return (p > threshold).sum(axis=1)\n\n# Calcoli\nsvi_entropy = entropy(svi_batch)\nmcmc_entropy = entropy(mcmc_mean_batch)\n\nsvi_active = active_topics(svi_batch)\nmcmc_active = active_topics(mcmc_mean_batch)\n\nmcmc_uncertainty = mcmc_std_batch.mean(axis=1)\n\n# === RISULTATI AGGREGATI ===\nprint(\"📊 METRICHE MEDIE SU 10 DOCUMENTI:\\n\")\nprint(f\"Entropia SVI         : {svi_entropy.mean():.4f}\")\nprint(f\"Entropia MCMC        : {mcmc_entropy.mean():.4f}\")\nprint(f\"Topic attivi SVI     : {svi_active.mean():.2f}\")\nprint(f\"Topic attivi MCMC    : {mcmc_active.mean():.2f}\")\nprint(f\"Incertezza MCMC (std): {mcmc_uncertainty.mean():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nplt.plot(svi_entropy, label=\"SVI\", marker='o')\nplt.plot(mcmc_entropy, label=\"MCMC\", marker='x')\nplt.title(\"Entropia per documento (SVI vs MCMC)\")\nplt.xlabel(\"Documento\")\nplt.ylabel(\"Entropia\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 4))\nplt.plot(svi_active, label=\"SVI\", marker='o')\nplt.plot(mcmc_active, label=\"MCMC\", marker='x')\nplt.title(\"Topic attivi (>0.05) per documento\")\nplt.xlabel(\"Documento\")\nplt.ylabel(\"# topic attivi\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.boxplot(mcmc_std_batch.mean(axis=1), vert=False)\nplt.title(\"Distribuzione dell’incertezza media per documento (MCMC)\")\nplt.xlabel(\"Varianza media dei topic\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Start with Bayesian Classifier","metadata":{}},{"cell_type":"code","source":"df_k60 = pd.read_csv(\"/kaggle/working/doc_topics_train_K60.csv\")\ndf_sms = pd.read_csv(\n    \"/kaggle/input/smsdataset/SMSSpamCollection\", sep=\"\\t\", header=None, names=[\"label\", \"message\"]\n)\n# df_sms.head()\ndf_sms[\"label\"] = df_sms[\"label\"].map({\"ham\": 0, \"spam\": 1})\ndf_k60[\"label\"] = df_sms[\"label\"]\nbow_array = np.load(\"/kaggle/input/bow-xarray/BoW_X_Array.npz\")[\"arr_0\"]\ndf_k60.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T12:17:52.241309Z","iopub.execute_input":"2025-05-16T12:17:52.241595Z","iopub.status.idle":"2025-05-16T12:17:52.733995Z","shell.execute_reply.started":"2025-05-16T12:17:52.241576Z","shell.execute_reply":"2025-05-16T12:17:52.733439Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"        topic_0       topic_1       topic_2       topic_3       topic_4  \\\n0  1.000000e+00  5.490356e-12  5.500995e-12  5.935029e-12  5.934962e-12   \n1  1.120545e-13  1.262004e-13  1.266233e-13  6.726096e-09  1.926159e-08   \n2  3.464618e-13  2.145838e-08  1.414600e-07  4.427267e-13  4.429049e-13   \n3  9.117645e-14  1.053106e-13  1.053608e-13  3.049746e-09  2.115192e-08   \n4  6.902157e-14  8.031758e-14  8.042826e-14  2.778378e-09  2.164246e-08   \n\n        topic_5       topic_6       topic_7       topic_8       topic_9  ...  \\\n0  5.500596e-12  5.501006e-12  5.934859e-12  5.935007e-12  5.500953e-12  ...   \n1  1.266194e-13  1.266245e-13  9.999993e-01  2.872921e-08  1.266240e-13  ...   \n2  5.805587e-05  2.683781e-07  4.431238e-13  4.431956e-13  4.698514e-05  ...   \n3  1.053258e-13  1.053269e-13  4.779859e-08  1.177722e-06  1.053773e-13  ...   \n4  8.034501e-14  8.034746e-14  1.424361e-08  2.850565e-07  8.034730e-14  ...   \n\n       topic_51      topic_52      topic_53      topic_54      topic_55  \\\n0  5.935007e-12  5.934996e-12  5.500953e-12  5.500963e-12  5.935007e-12   \n1  2.901031e-08  2.895216e-08  1.266242e-13  1.266242e-13  2.901031e-08   \n2  4.431956e-13  4.431956e-13  5.818885e-03  6.797621e-03  4.431956e-13   \n3  8.680254e-06  4.983503e-05  1.053773e-13  1.053779e-13  8.680262e-06   \n4  1.413670e-05  1.250400e-05  8.034715e-14  8.034715e-14  1.413582e-05   \n\n       topic_56      topic_57      topic_58      topic_59  label  \n0  5.934996e-12  5.500953e-12  5.500963e-12  5.935007e-12      0  \n1  2.895221e-08  1.266242e-13  1.266242e-13  2.901031e-08      0  \n2  4.431956e-13  6.871266e-03  6.662033e-03  4.431956e-13      1  \n3  5.056385e-05  1.053773e-13  1.053779e-13  8.680246e-06      0  \n4  1.276869e-05  8.034715e-14  8.034715e-14  1.413578e-05      0  \n\n[5 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic_0</th>\n      <th>topic_1</th>\n      <th>topic_2</th>\n      <th>topic_3</th>\n      <th>topic_4</th>\n      <th>topic_5</th>\n      <th>topic_6</th>\n      <th>topic_7</th>\n      <th>topic_8</th>\n      <th>topic_9</th>\n      <th>...</th>\n      <th>topic_51</th>\n      <th>topic_52</th>\n      <th>topic_53</th>\n      <th>topic_54</th>\n      <th>topic_55</th>\n      <th>topic_56</th>\n      <th>topic_57</th>\n      <th>topic_58</th>\n      <th>topic_59</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000e+00</td>\n      <td>5.490356e-12</td>\n      <td>5.500995e-12</td>\n      <td>5.935029e-12</td>\n      <td>5.934962e-12</td>\n      <td>5.500596e-12</td>\n      <td>5.501006e-12</td>\n      <td>5.934859e-12</td>\n      <td>5.935007e-12</td>\n      <td>5.500953e-12</td>\n      <td>...</td>\n      <td>5.935007e-12</td>\n      <td>5.934996e-12</td>\n      <td>5.500953e-12</td>\n      <td>5.500963e-12</td>\n      <td>5.935007e-12</td>\n      <td>5.934996e-12</td>\n      <td>5.500953e-12</td>\n      <td>5.500963e-12</td>\n      <td>5.935007e-12</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.120545e-13</td>\n      <td>1.262004e-13</td>\n      <td>1.266233e-13</td>\n      <td>6.726096e-09</td>\n      <td>1.926159e-08</td>\n      <td>1.266194e-13</td>\n      <td>1.266245e-13</td>\n      <td>9.999993e-01</td>\n      <td>2.872921e-08</td>\n      <td>1.266240e-13</td>\n      <td>...</td>\n      <td>2.901031e-08</td>\n      <td>2.895216e-08</td>\n      <td>1.266242e-13</td>\n      <td>1.266242e-13</td>\n      <td>2.901031e-08</td>\n      <td>2.895221e-08</td>\n      <td>1.266242e-13</td>\n      <td>1.266242e-13</td>\n      <td>2.901031e-08</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.464618e-13</td>\n      <td>2.145838e-08</td>\n      <td>1.414600e-07</td>\n      <td>4.427267e-13</td>\n      <td>4.429049e-13</td>\n      <td>5.805587e-05</td>\n      <td>2.683781e-07</td>\n      <td>4.431238e-13</td>\n      <td>4.431956e-13</td>\n      <td>4.698514e-05</td>\n      <td>...</td>\n      <td>4.431956e-13</td>\n      <td>4.431956e-13</td>\n      <td>5.818885e-03</td>\n      <td>6.797621e-03</td>\n      <td>4.431956e-13</td>\n      <td>4.431956e-13</td>\n      <td>6.871266e-03</td>\n      <td>6.662033e-03</td>\n      <td>4.431956e-13</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9.117645e-14</td>\n      <td>1.053106e-13</td>\n      <td>1.053608e-13</td>\n      <td>3.049746e-09</td>\n      <td>2.115192e-08</td>\n      <td>1.053258e-13</td>\n      <td>1.053269e-13</td>\n      <td>4.779859e-08</td>\n      <td>1.177722e-06</td>\n      <td>1.053773e-13</td>\n      <td>...</td>\n      <td>8.680254e-06</td>\n      <td>4.983503e-05</td>\n      <td>1.053773e-13</td>\n      <td>1.053779e-13</td>\n      <td>8.680262e-06</td>\n      <td>5.056385e-05</td>\n      <td>1.053773e-13</td>\n      <td>1.053779e-13</td>\n      <td>8.680246e-06</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6.902157e-14</td>\n      <td>8.031758e-14</td>\n      <td>8.042826e-14</td>\n      <td>2.778378e-09</td>\n      <td>2.164246e-08</td>\n      <td>8.034501e-14</td>\n      <td>8.034746e-14</td>\n      <td>1.424361e-08</td>\n      <td>2.850565e-07</td>\n      <td>8.034730e-14</td>\n      <td>...</td>\n      <td>1.413670e-05</td>\n      <td>1.250400e-05</td>\n      <td>8.034715e-14</td>\n      <td>8.034715e-14</td>\n      <td>1.413582e-05</td>\n      <td>1.276869e-05</td>\n      <td>8.034715e-14</td>\n      <td>8.034715e-14</td>\n      <td>1.413578e-05</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 61 columns</p>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## BAYESIAN CLASSIFIER","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\nfrom pyro.infer.autoguide import AutoDelta\nfrom sklearn.metrics import classification_report, roc_auc_score\n\n# === LOAD DATASETS ===\nX_train = pd.read_csv(\"/kaggle/working/doc_topics_train_K60.csv\").values\nX_test = pd.read_csv(\"/kaggle/working/doc_topics_test_K60_mcmc_mean.csv\").values\n\ny_train = (pd.read_csv(\"sms_train.csv\")['label'] == 'spam').astype(float).values\ny_test = (pd.read_csv(\"sms_test.csv\")['label'] == 'spam').astype(float).values\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train, dtype=torch.float)\nX_test = torch.tensor(X_test, dtype=torch.float)\ny_train = torch.tensor(y_train, dtype=torch.float)\ny_test = torch.tensor(y_test, dtype=torch.float)\n\n# === MODEL ===\ndef model(X, y=None):\n    num_features = X.shape[1]\n    beta = pyro.sample(\"beta\", dist.Normal(torch.zeros(num_features), torch.ones(num_features)).to_event(1))\n    bias = pyro.sample(\"bias\", dist.Normal(0., 1.))\n    with pyro.plate(\"data\", X.shape[0]):\n        logits = (X @ beta) + bias\n        pyro.sample(\"obs\", dist.Bernoulli(logits=logits), obs=y)\n\n\n# === SVI INFERENCE ===\npyro.clear_param_store()\nguide = AutoDelta(model)\nsvi = SVI(model, guide, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n\nfor step in range(1000):\n    loss = svi.step(X_train, y_train)\n    if step % 100 == 0:\n        print(f\"[SVI step {step}] loss = {loss:.2f}\")\n\nposterior = guide(X_train, y_train)\nweights = posterior[\"beta\"]\nbias = posterior[\"bias\"]\n\n# Predictions\nlogits_test = (X_test @ weights) + bias\nprobs_test = torch.sigmoid(logits_test).detach().numpy()\npred_test = (probs_test > 0.5).astype(int)\n\n# Evaluation\nprint(\"\\n=== Evaluation on test set ===\")\nprint(classification_report(y_test, pred_test, target_names=[\"ham\", \"spam\"]))\nprint(f\"AUC: {roc_auc_score(y_test, probs_test):.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:26:29.168535Z","iopub.execute_input":"2025-05-16T13:26:29.169166Z","iopub.status.idle":"2025-05-16T13:26:32.192680Z","shell.execute_reply.started":"2025-05-16T13:26:29.169147Z","shell.execute_reply":"2025-05-16T13:26:32.191838Z"}},"outputs":[{"name":"stdout","text":"[SVI step 0] loss = 2546.42\n[SVI step 100] loss = 1829.73\n[SVI step 200] loss = 1808.07\n[SVI step 300] loss = 1804.87\n[SVI step 400] loss = 1803.15\n[SVI step 500] loss = 1801.75\n[SVI step 600] loss = 1800.54\n[SVI step 700] loss = 1799.52\n[SVI step 800] loss = 1798.68\n[SVI step 900] loss = 1798.02\n\n=== Evaluation on test set ===\n              precision    recall  f1-score   support\n\n         ham       0.87      1.00      0.93       966\n        spam       0.00      0.00      0.00       149\n\n    accuracy                           0.87      1115\n   macro avg       0.43      0.50      0.46      1115\nweighted avg       0.75      0.87      0.80      1115\n\nAUC: 0.483\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}