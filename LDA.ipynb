{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098c64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d425d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "df = pd.read_csv(\"lda_df.csv\")\n",
    "\n",
    "with open(\"lda_vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "X_array = np.load(\"BoW_X_Array.npz\")[\"arr_0\"]\n",
    "X_tensor = torch.tensor(X_array, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95735e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set model parameters ---\n",
    "K = 30\n",
    "num_docs, vocab_size = X_tensor.shape\n",
    "\n",
    "\n",
    "# --- Define Pyro LDA model ---\n",
    "def lda_model(data):\n",
    "    with pyro.plate(\"topics\", K):\n",
    "        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(vocab_size)))\n",
    "\n",
    "    with pyro.plate(\"documents\", num_docs):\n",
    "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(K)))\n",
    "\n",
    "        word_dists = torch.matmul(doc_topics, topic_words)\n",
    "        word_dists = word_dists / word_dists.sum(dim=1, keepdim=True)\n",
    "\n",
    "        total_count = 100\n",
    "        pyro.sample(\n",
    "            \"doc_words\",\n",
    "            dist.Multinomial(total_count=total_count, probs=word_dists),\n",
    "            obs=data,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35277ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up SVI ---\n",
    "guide = AutoDelta(lda_model)\n",
    "optimizer = pyro.optim.Adam({\"lr\": 0.01})\n",
    "svi = SVI(lda_model, guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9c7e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0] loss = -645081.38\n",
      "[step 10] loss = -649994.19\n",
      "[step 20] loss = -654594.69\n",
      "[step 30] loss = -659034.31\n",
      "[step 40] loss = -663410.34\n",
      "[step 50] loss = -667763.56\n",
      "[step 60] loss = -672065.88\n",
      "[step 70] loss = -676251.25\n",
      "[step 80] loss = -680276.12\n",
      "[step 90] loss = -684130.19\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop ---\n",
    "num_steps = 100\n",
    "for step in range(num_steps):\n",
    "    loss = svi.step(X_tensor)\n",
    "    if step % 10 == 0:\n",
    "        print(f\"[step {step}] loss = {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0caccb0",
   "metadata": {},
   "source": [
    "Il tuo training Ã¨ assolutamente in linea con le aspettative. Il loss parte da -645k perchÃ© stai modellando tanti documenti e parole contemporaneamente, e Pyro somma tutto.\n",
    "\n",
    "Fammi sapere se vuoi normalizzare, loggare o tracciare il progresso del training visivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614207ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract results ---\n",
    "posterior = guide()\n",
    "topic_words = posterior[\"topic_words\"]\n",
    "doc_topics = posterior[\"doc_topics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ecb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTIONAL: Save topic vectors per doc ---\n",
    "topic_df = pd.DataFrame(\n",
    "    doc_topics.detach().numpy(), columns=[f\"topic_{i}\" for i in range(K)]\n",
    ")\n",
    "df_with_topics = pd.concat([df.reset_index(drop=True), topic_df], axis=1)\n",
    "df_with_topics.to_csv(\"lda_output_with_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bd4a3",
   "metadata": {},
   "source": [
    "### Topic Number Selection (K)\n",
    "\n",
    "Since the number of topics `K` in **Latent Dirichlet Allocation (LDA)** must be specified in advance, we ran the Pyro-based LDA model across multiple candidate values (e.g. K = 5, 10, 15, ..., 30) and monitored the Evidence Lower Bound (ELBO) loss during training. \n",
    "\n",
    "For each value of `K`, we trained the model for a (smaller) fixed number of steps and recorded the final loss. The model with the lowest ELBO loss was selected as the best configuration. This approach allows us to balance model complexity and fit without relying on manual inspection or external coherence metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa356657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LDA with K = 5 ---\n",
      "[K=5 | step 0] loss = 51524.04\n",
      "[K=5 | step 10] loss = 45766.19\n",
      "[K=5 | step 20] loss = 40301.16\n",
      "[K=5 | step 30] loss = 35131.79\n",
      "[K=5 | step 40] loss = 30212.57\n",
      "[K=5 | step 50] loss = 25451.38\n",
      "[K=5 | step 60] loss = 20725.76\n",
      "[K=5 | step 70] loss = 15940.26\n",
      "[K=5 | step 80] loss = 11106.19\n",
      "[K=5 | step 90] loss = 6364.44\n",
      "[K=5] final loss = 2336.94\n",
      "âœ… New best K = 5\n",
      "\n",
      "--- Training LDA with K = 10 ---\n",
      "[K=10 | step 0] loss = -281346.40\n",
      "[K=10 | step 10] loss = -287104.27\n",
      "[K=10 | step 20] loss = -292569.77\n",
      "[K=10 | step 30] loss = -297743.02\n",
      "[K=10 | step 40] loss = -302670.43\n",
      "[K=10 | step 50] loss = -307437.62\n",
      "[K=10 | step 60] loss = -312155.18\n",
      "[K=10 | step 70] loss = -316923.46\n",
      "[K=10 | step 80] loss = -321781.40\n",
      "[K=10 | step 90] loss = -326667.27\n",
      "[K=10] final loss = -330968.90\n",
      "âœ… New best K = 10\n",
      "\n",
      "--- Training LDA with K = 15 ---\n",
      "[K=15 | step 0] loss = -629626.97\n",
      "[K=15 | step 10] loss = -635384.88\n",
      "[K=15 | step 20] loss = -640849.72\n",
      "[K=15 | step 30] loss = -646009.62\n",
      "[K=15 | step 40] loss = -650887.03\n",
      "[K=15 | step 50] loss = -655541.50\n",
      "[K=15 | step 60] loss = -660056.59\n",
      "[K=15 | step 70] loss = -664505.34\n",
      "[K=15 | step 80] loss = -668913.88\n",
      "[K=15 | step 90] loss = -673246.31\n",
      "[K=15] final loss = -677031.38\n",
      "âœ… New best K = 15\n",
      "\n",
      "--- Training LDA with K = 20 ---\n",
      "[K=20 | step 0] loss = -987709.91\n",
      "[K=20 | step 10] loss = -993468.56\n",
      "[K=20 | step 20] loss = -998942.81\n",
      "[K=20 | step 30] loss = -1004137.56\n",
      "[K=20 | step 40] loss = -1009093.25\n",
      "[K=20 | step 50] loss = -1013880.84\n",
      "[K=20 | step 60] loss = -1018595.94\n",
      "[K=20 | step 70] loss = -1023315.00\n",
      "[K=20 | step 80] loss = -1028031.44\n",
      "[K=20 | step 90] loss = -1032634.16\n",
      "[K=20] final loss = -1036552.28\n",
      "âœ… New best K = 20\n",
      "\n",
      "--- Training LDA with K = 25 ---\n",
      "[K=25 | step 0] loss = -1353015.34\n",
      "[K=25 | step 10] loss = -1358773.19\n",
      "[K=25 | step 20] loss = -1364236.81\n",
      "[K=25 | step 30] loss = -1369391.06\n",
      "[K=25 | step 40] loss = -1374251.59\n",
      "[K=25 | step 50] loss = -1378860.22\n",
      "[K=25 | step 60] loss = -1383269.69\n",
      "[K=25 | step 70] loss = -1387524.50\n",
      "[K=25 | step 80] loss = -1391644.31\n",
      "[K=25 | step 90] loss = -1395621.09\n",
      "[K=25] final loss = -1399064.31\n",
      "âœ… New best K = 25\n",
      "\n",
      "--- Training LDA with K = 30 ---\n",
      "[K=30 | step 0] loss = -1724045.72\n",
      "[K=30 | step 10] loss = -1729803.56\n",
      "[K=30 | step 20] loss = -1735266.53\n",
      "[K=30 | step 30] loss = -1740417.75\n",
      "[K=30 | step 40] loss = -1745273.91\n",
      "[K=30 | step 50] loss = -1749888.81\n",
      "[K=30 | step 60] loss = -1754361.72\n",
      "[K=30 | step 70] loss = -1758835.34\n",
      "[K=30 | step 80] loss = -1763477.81\n",
      "[K=30 | step 90] loss = -1768421.41\n",
      "[K=30] final loss = -1773109.19\n",
      "âœ… New best K = 30\n",
      "\n",
      "ðŸŽ¯ Best K = 30 with loss = -1773109.19\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "K_values = [5, 10, 15, 20, 25, 30]\n",
    "best_loss = float(\"inf\")\n",
    "best_K = None\n",
    "best_outputs = {}\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n--- Training LDA with K = {K} ---\")\n",
    "\n",
    "    num_docs, vocab_size = X_tensor.shape\n",
    "\n",
    "    def lda_model(data):\n",
    "        with pyro.plate(\"topics\", K):\n",
    "            topic_words = pyro.sample(\n",
    "                \"topic_words\", dist.Dirichlet(torch.ones(vocab_size))\n",
    "            )\n",
    "\n",
    "        with pyro.plate(\"documents\", num_docs):\n",
    "            doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(K)))\n",
    "            word_dists = torch.matmul(doc_topics, topic_words)\n",
    "            word_dists = word_dists / word_dists.sum(dim=1, keepdim=True)\n",
    "            pyro.sample(\n",
    "                \"doc_words\",\n",
    "                dist.Multinomial(total_count=100, probs=word_dists),\n",
    "                obs=data,\n",
    "            )\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    guide = AutoDelta(lda_model)\n",
    "    svi = SVI(lda_model, guide, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "\n",
    "    num_steps = 100\n",
    "    loss = None\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(X_tensor)\n",
    "        if step % 10 == 0:\n",
    "            print(f\"[K={K} | step {step}] loss = {loss:.2f}\")\n",
    "\n",
    "    print(f\"[K={K}] final loss = {loss:.2f}\")\n",
    "\n",
    "    if loss < best_loss:\n",
    "        print(f\"âœ… New best K = {K}\")\n",
    "        best_loss = loss\n",
    "        best_K = K\n",
    "        posterior = guide()\n",
    "        best_outputs = {\n",
    "            \"K\": K,\n",
    "            \"loss\": loss,\n",
    "            \"doc_topics\": posterior[\"doc_topics\"].detach().clone(),\n",
    "            \"topic_words\": posterior[\"topic_words\"].detach().clone(),\n",
    "        }\n",
    "\n",
    "# Save the best output\n",
    "print(f\"\\nðŸŽ¯ Best K = {best_K} with loss = {best_loss:.2f}\")\n",
    "doc_topics_df = pd.DataFrame(\n",
    "    best_outputs[\"doc_topics\"].numpy(), columns=[f\"topic_{i}\" for i in range(best_K)]\n",
    ")\n",
    "df_with_topics = pd.concat([df.reset_index(drop=True), doc_topics_df], axis=1)\n",
    "df_with_topics.to_csv(f\"best_lda_doc_topics_K{best_K}.csv\", index=False)\n",
    "torch.save(best_outputs[\"topic_words\"], f\"best_topic_words_K{best_K}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087504cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LDA with K = 40 ---\n",
      "[K=40 | step 0] loss = -2479647.09\n",
      "[K=40 | step 1] loss = -2480235.25\n",
      "[K=40 | step 2] loss = -2480820.69\n",
      "[K=40 | step 3] loss = -2481403.41\n",
      "[K=40 | step 4] loss = -2481983.34\n",
      "[K=40 | step 5] loss = -2482560.62\n",
      "[K=40 | step 6] loss = -2483135.16\n",
      "[K=40 | step 7] loss = -2483707.03\n",
      "[K=40 | step 8] loss = -2484276.09\n",
      "[K=40 | step 9] loss = -2484842.38\n",
      "[K=40] final loss = -2484842.38\n",
      "âœ… New best K = 40\n",
      "\n",
      "--- Training LDA with K = 50 ---\n",
      "[K=50 | step 0] loss = -3249508.47\n",
      "[K=50 | step 1] loss = -3250096.62\n",
      "[K=50 | step 2] loss = -3250682.06\n",
      "[K=50 | step 3] loss = -3251264.78\n",
      "[K=50 | step 4] loss = -3251844.69\n",
      "[K=50 | step 5] loss = -3252421.91\n",
      "[K=50 | step 6] loss = -3252996.44\n",
      "[K=50 | step 7] loss = -3253568.19\n",
      "[K=50 | step 8] loss = -3254137.19\n",
      "[K=50 | step 9] loss = -3254703.28\n",
      "[K=50] final loss = -3254703.28\n",
      "âœ… New best K = 50\n",
      "\n",
      "--- Training LDA with K = 60 ---\n",
      "[K=60 | step 0] loss = -4030704.59\n",
      "[K=60 | step 1] loss = -4031292.75\n",
      "[K=60 | step 2] loss = -4031878.19\n",
      "[K=60 | step 3] loss = -4032460.91\n",
      "[K=60 | step 4] loss = -4033040.78\n",
      "[K=60 | step 5] loss = -4033618.00\n",
      "[K=60 | step 6] loss = -4034192.50\n",
      "[K=60 | step 7] loss = -4034764.25\n",
      "[K=60 | step 8] loss = -4035333.19\n",
      "[K=60 | step 9] loss = -4035899.28\n",
      "[K=60] final loss = -4035899.28\n",
      "âœ… New best K = 60\n",
      "\n",
      "--- Training LDA with K = 70 ---\n",
      "[K=70 | step 0] loss = -4821309.09\n",
      "[K=70 | step 1] loss = -4821897.28\n",
      "[K=70 | step 2] loss = -4822482.69\n",
      "[K=70 | step 3] loss = -4823065.41\n",
      "[K=70 | step 4] loss = -4823645.31\n",
      "[K=70 | step 5] loss = -4824222.53\n",
      "[K=70 | step 6] loss = -4824797.09\n",
      "[K=70 | step 7] loss = -4825368.88\n",
      "[K=70 | step 8] loss = -4825937.84\n",
      "[K=70 | step 9] loss = -4826504.03\n",
      "[K=70] final loss = -4826504.03\n",
      "âœ… New best K = 70\n",
      "\n",
      "--- Training LDA with K = 80 ---\n",
      "[K=80 | step 0] loss = -5619958.59\n",
      "[K=80 | step 1] loss = -5620546.75\n",
      "[K=80 | step 2] loss = -5621132.19\n",
      "[K=80 | step 3] loss = -5621714.91\n",
      "[K=80 | step 4] loss = -5622294.81\n",
      "[K=80 | step 5] loss = -5622872.03\n",
      "[K=80 | step 6] loss = -5623446.59\n",
      "[K=80 | step 7] loss = -5624018.38\n",
      "[K=80 | step 8] loss = -5624587.38\n",
      "[K=80 | step 9] loss = -5625153.56\n",
      "[K=80] final loss = -5625153.56\n",
      "âœ… New best K = 80\n",
      "\n",
      "--- Training LDA with K = 90 ---\n",
      "[K=90 | step 0] loss = -6425635.59\n",
      "[K=90 | step 1] loss = -6426223.75\n",
      "[K=90 | step 2] loss = -6426809.19\n",
      "[K=90 | step 3] loss = -6427391.91\n",
      "[K=90 | step 4] loss = -6427971.81\n",
      "[K=90 | step 5] loss = -6428549.00\n",
      "[K=90 | step 6] loss = -6429123.50\n",
      "[K=90 | step 7] loss = -6429695.28\n",
      "[K=90 | step 8] loss = -6430264.25\n",
      "[K=90 | step 9] loss = -6430830.31\n",
      "[K=90] final loss = -6430830.31\n",
      "âœ… New best K = 90\n",
      "\n",
      "ðŸŽ¯ Best K = 90 with loss = -6430830.31\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "K_values = [40, 50, 60, 70, 80, 90]\n",
    "best_loss = float(\"inf\")\n",
    "best_K = None\n",
    "best_outputs = {}\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n--- Training LDA with K = {K} ---\")\n",
    "\n",
    "    num_docs, vocab_size = X_tensor.shape\n",
    "\n",
    "    def lda_model(data):\n",
    "        with pyro.plate(\"topics\", K):\n",
    "            topic_words = pyro.sample(\n",
    "                \"topic_words\", dist.Dirichlet(torch.ones(vocab_size))\n",
    "            )\n",
    "\n",
    "        with pyro.plate(\"documents\", num_docs):\n",
    "            doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(K)))\n",
    "            word_dists = torch.matmul(doc_topics, topic_words)\n",
    "            word_dists = word_dists / word_dists.sum(dim=1, keepdim=True)\n",
    "            pyro.sample(\n",
    "                \"doc_words\",\n",
    "                dist.Multinomial(total_count=100, probs=word_dists),\n",
    "                obs=data,\n",
    "            )\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    guide = AutoDelta(lda_model)\n",
    "    svi = SVI(lda_model, guide, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "\n",
    "    num_steps = 10\n",
    "    loss = None\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(X_tensor)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"[K={K} | step {step}] loss = {loss:.2f}\")\n",
    "\n",
    "    print(f\"[K={K}] final loss = {loss:.2f}\")\n",
    "\n",
    "    if loss < best_loss:\n",
    "        print(f\"âœ… New best K = {K}\")\n",
    "        best_loss = loss\n",
    "        best_K = K\n",
    "        posterior = guide()\n",
    "        best_outputs = {\n",
    "            \"K\": K,\n",
    "            \"loss\": loss,\n",
    "            \"doc_topics\": posterior[\"doc_topics\"].detach().clone(),\n",
    "            \"topic_words\": posterior[\"topic_words\"].detach().clone(),\n",
    "        }\n",
    "\n",
    "# Save the best output\n",
    "print(f\"\\nðŸŽ¯ Best K = {best_K} with loss = {best_loss:.2f}\")\n",
    "doc_topics_df = pd.DataFrame(\n",
    "    best_outputs[\"doc_topics\"].numpy(), columns=[f\"topic_{i}\" for i in range(best_K)]\n",
    ")\n",
    "df_with_topics = pd.concat([df.reset_index(drop=True), doc_topics_df], axis=1)\n",
    "df_with_topics.to_csv(f\"best_lda_doc_topics_K{best_K}.csv\", index=False)\n",
    "torch.save(best_outputs[\"topic_words\"], f\"best_topic_words_K{best_K}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training LDA with K = 5 ---\n",
      "\n",
      "--- Training LDA with K = 10 ---\n",
      "\n",
      "--- Training LDA with K = 15 ---\n",
      "\n",
      "--- Training LDA with K = 20 ---\n",
      "\n",
      "--- Training LDA with K = 25 ---\n",
      "\n",
      "--- Training LDA with K = 30 ---\n",
      "\n",
      "--- Training LDA with K = 35 ---\n",
      "\n",
      "--- Training LDA with K = 40 ---\n",
      "\n",
      "--- Training LDA with K = 45 ---\n",
      "\n",
      "--- Training LDA with K = 50 ---\n",
      "\n",
      "--- Training LDA with K = 55 ---\n",
      "\n",
      "--- Training LDA with K = 60 ---\n",
      "\n",
      "--- Training LDA with K = 65 ---\n",
      "\n",
      "--- Training LDA with K = 70 ---\n",
      "\n",
      "--- Training LDA with K = 75 ---\n",
      "\n",
      "--- Training LDA with K = 80 ---\n",
      "\n",
      "--- Training LDA with K = 85 ---\n",
      "\n",
      "--- Training LDA with K = 90 ---\n",
      "\n",
      "ðŸ“Š Risultati confronto K:\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\compat\\_optional.py:132\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tabulate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š Risultati confronto K:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:2839\u001b[0m, in \u001b[0;36mDataFrame.to_markdown\u001b[1;34m(self, buf, mode, index, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2837\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtablefmt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2838\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshowindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, index)\n\u001b[1;32m-> 2839\u001b[0m tabulate \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtabulate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2840\u001b[0m result \u001b[38;5;241m=\u001b[39m tabulate\u001b[38;5;241m.\u001b[39mtabulate(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\compat\\_optional.py:135\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'tabulate'.  Use pip or conda to install tabulate."
     ]
    }
   ],
   "source": [
    "# Carica la matrice documento-termine\n",
    "X_array = np.load(\"BoW_X_Array.npz\")[\"arr_0\"]\n",
    "X_tensor = torch.tensor(X_array, dtype=torch.float)\n",
    "\n",
    "num_docs, vocab_size = X_tensor.shape\n",
    "\n",
    "# Range di valori K da testare\n",
    "K_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90]\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n--- Training LDA with K = {K} ---\")\n",
    "\n",
    "    def lda_model(data):\n",
    "        with pyro.plate(\"topics\", K):\n",
    "            topic_words = pyro.sample(\n",
    "                \"topic_words\", dist.Dirichlet(torch.ones(vocab_size))\n",
    "            )\n",
    "        with pyro.plate(\"documents\", num_docs):\n",
    "            doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(K)))\n",
    "            word_dists = torch.matmul(doc_topics, topic_words)\n",
    "            word_dists = word_dists / word_dists.sum(dim=1, keepdim=True)\n",
    "            pyro.sample(\n",
    "                \"doc_words\",\n",
    "                dist.Multinomial(total_count=100, probs=word_dists),\n",
    "                obs=data,\n",
    "            )\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    guide = AutoDelta(lda_model)\n",
    "    svi = SVI(lda_model, guide, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "\n",
    "    # Solo 10 passi di training\n",
    "    for step in range(10):\n",
    "        loss = svi.step(X_tensor)\n",
    "\n",
    "    # Estrai i risultati\n",
    "    posterior = guide()\n",
    "    doc_topics = posterior[\"doc_topics\"]\n",
    "    topic_usage = doc_topics.sum(dim=0).detach().numpy()\n",
    "    threshold = 5.0\n",
    "    num_active_topics = (topic_usage > threshold).sum()\n",
    "\n",
    "    results.append(\n",
    "        {\"K\": K, \"Final Loss\": float(loss), \"Active Topics\": int(num_active_topics)}\n",
    "    )\n",
    "\n",
    "# Mostra i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Risultati confronto K:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1792f8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Final Loss</th>\n",
       "      <th>Active Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4.632935e+04</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-2.865411e+05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>-6.348217e+05</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>-9.929052e+05</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>-1.358210e+06</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>-1.729240e+06</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>-2.105015e+06</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>-2.484842e+06</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45</td>\n",
       "      <td>-2.868204e+06</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>-3.254703e+06</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>-3.644022e+06</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>-4.035899e+06</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>65</td>\n",
       "      <td>-4.430121e+06</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70</td>\n",
       "      <td>-4.826504e+06</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75</td>\n",
       "      <td>-5.224892e+06</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80</td>\n",
       "      <td>-5.625154e+06</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>85</td>\n",
       "      <td>-6.027168e+06</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90</td>\n",
       "      <td>-6.430830e+06</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     K    Final Loss  Active Topics\n",
       "0    5  4.632935e+04              5\n",
       "1   10 -2.865411e+05             10\n",
       "2   15 -6.348217e+05             15\n",
       "3   20 -9.929052e+05             20\n",
       "4   25 -1.358210e+06             25\n",
       "5   30 -1.729240e+06             30\n",
       "6   35 -2.105015e+06             35\n",
       "7   40 -2.484842e+06             40\n",
       "8   45 -2.868204e+06             45\n",
       "9   50 -3.254703e+06             50\n",
       "10  55 -3.644022e+06             55\n",
       "11  60 -4.035899e+06             60\n",
       "12  65 -4.430121e+06             65\n",
       "13  70 -4.826504e+06             70\n",
       "14  75 -5.224892e+06             75\n",
       "15  80 -5.625154e+06             80\n",
       "16  85 -6.027168e+06             85\n",
       "17  90 -6.430830e+06             90"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
