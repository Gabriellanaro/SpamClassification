{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d425d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "df = pd.read_csv(\"lda_df.csv\")\n",
    "\n",
    "with open(\"lda_vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "X_array = np.load(\"BoW_X_Array.npz\")[\"arr_0\"]\n",
    "X_tensor = torch.tensor(X_array, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95735e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set model parameters ---\n",
    "K = 30\n",
    "num_docs, vocab_size = X_tensor.shape\n",
    "\n",
    "\n",
    "# --- Define Pyro LDA model ---\n",
    "def lda_model(data):\n",
    "    with pyro.plate(\"topics\", K):\n",
    "        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(vocab_size)))\n",
    "\n",
    "    with pyro.plate(\"documents\", num_docs):\n",
    "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(K)))\n",
    "\n",
    "        word_dists = torch.matmul(doc_topics, topic_words)\n",
    "        word_dists = word_dists / word_dists.sum(dim=1, keepdim=True)\n",
    "\n",
    "        total_count = 100\n",
    "        pyro.sample(\n",
    "            \"doc_words\",\n",
    "            dist.Multinomial(total_count=total_count, probs=word_dists),\n",
    "            obs=data,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35277ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up SVI ---\n",
    "guide = AutoDelta(lda_model)\n",
    "optimizer = pyro.optim.Adam({\"lr\": 0.01})\n",
    "svi = SVI(lda_model, guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9c7e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0] loss = -645081.38\n",
      "[step 10] loss = -649994.19\n",
      "[step 20] loss = -654594.69\n",
      "[step 30] loss = -659034.31\n",
      "[step 40] loss = -663410.34\n",
      "[step 50] loss = -667763.56\n",
      "[step 60] loss = -672065.88\n",
      "[step 70] loss = -676251.25\n",
      "[step 80] loss = -680276.12\n",
      "[step 90] loss = -684130.19\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop ---\n",
    "num_steps = 100\n",
    "for step in range(num_steps):\n",
    "    loss = svi.step(X_tensor)\n",
    "    if step % 10 == 0:\n",
    "        print(f\"[step {step}] loss = {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0caccb0",
   "metadata": {},
   "source": [
    "Il tuo training Ã¨ assolutamente in linea con le aspettative. Il loss parte da -645k perchÃ© stai modellando tanti documenti e parole contemporaneamente, e Pyro somma tutto.\n",
    "\n",
    "Fammi sapere se vuoi normalizzare, loggare o tracciare il progresso del training visivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614207ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract results ---\n",
    "posterior = guide()\n",
    "topic_words = posterior[\"topic_words\"]\n",
    "doc_topics = posterior[\"doc_topics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24ecb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OPTIONAL: Save topic vectors per doc ---\n",
    "topic_df = pd.DataFrame(\n",
    "    doc_topics.detach().numpy(), columns=[f\"topic_{i}\" for i in range(K)]\n",
    ")\n",
    "df_with_topics = pd.concat([df.reset_index(drop=True), topic_df], axis=1)\n",
    "df_with_topics.to_csv(\"lda_output_with_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bd4a3",
   "metadata": {},
   "source": [
    "### Topic Number Selection (K)\n",
    "\n",
    "Since the number of topics `K` in **Latent Dirichlet Allocation (LDA)** must be specified in advance, we ran the Pyro-based LDA model across multiple candidate values (e.g. K = 5, 10, 15, ..., 30) and monitored the Evidence Lower Bound (ELBO) loss during training. \n",
    "\n",
    "For each value of `K`, we trained the model for a (smaller) fixed number of steps and recorded the final loss. The model with the lowest ELBO loss was selected as the best configuration. This approach allows us to balance model complexity and fit without relying on manual inspection or external coherence metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica i dati\n",
    "X_array = np.load(\"/kaggle/input/bow-xarray/BoW_X_Array.npz\")[\"arr_0\"]\n",
    "X_tensor = torch.tensor(X_array, dtype=torch.float)\n",
    "print(\"XArray added\")\n",
    "\n",
    "# Opzionale: usa GPU se disponibile\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "X_tensor = X_tensor.to(device)\n",
    "\n",
    "num_docs, vocab_size = X_tensor.shape\n",
    "K_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90]\n",
    "results = []\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"\\n--- Training LDA with K = {K} ---\")\n",
    "\n",
    "    def lda_model(data):\n",
    "        with pyro.plate(\"topics\", K):\n",
    "            topic_words = pyro.sample(\n",
    "                \"topic_words\", dist.Dirichlet(torch.ones(vocab_size).to(device))\n",
    "            )\n",
    "        with pyro.plate(\"documents\", num_docs):\n",
    "            doc_topics = pyro.sample(\n",
    "                \"doc_topics\", dist.Dirichlet(torch.ones(K).to(device))\n",
    "            )\n",
    "            word_dists = torch.matmul(doc_topics, topic_words)\n",
    "            logits = torch.matmul(doc_topics, topic_words).log()\n",
    "            pyro.sample(\n",
    "                \"doc_words\", dist.Multinomial(total_count=100, logits=logits), obs=data\n",
    "            )\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    guide = AutoDelta(lda_model)\n",
    "    svi = SVI(lda_model, guide, pyro.optim.Adam({\"lr\": 0.01}), loss=Trace_ELBO())\n",
    "\n",
    "    for step in range(500):\n",
    "        loss = svi.step(X_tensor)\n",
    "\n",
    "    posterior = guide()\n",
    "    doc_topics = posterior[\"doc_topics\"]\n",
    "    topic_usage = doc_topics.sum(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    # Statistiche extra\n",
    "    loss_per_doc = loss / num_docs\n",
    "    entropy = -(doc_topics * doc_topics.log()).sum(dim=1).mean().item()\n",
    "    avg_active_per_doc = (doc_topics > 0.05).sum(dim=1).float().mean().item()\n",
    "    num_active_topics = (topic_usage > 5.0).sum()\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"K\": K,\n",
    "            \"Final Loss\": float(loss),\n",
    "            \"Loss per Doc\": float(loss_per_doc),\n",
    "            \"Entropy\": float(entropy),\n",
    "            \"Avg Active Topics/Doc\": float(avg_active_per_doc),\n",
    "            \"Active Topics (global)\": int(num_active_topics),\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"results_k_selection.csv\", index=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Risultati confronto K:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1792f8f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the topic proportions matrix\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m doc_topics \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_topics_K60.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# or full path if needed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m topic_matrix \u001b[38;5;241m=\u001b[39m doc_topics\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# shape: [n_documents, K]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1. Average number of strong topics per document (threshold > 0.05)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the topic proportions matrix\n",
    "doc_topics = pd.read_csv(\"doc_topics_K60.csv\")  # or full path if needed\n",
    "topic_matrix = doc_topics.values  # shape: [n_documents, K]\n",
    "\n",
    "# 1. Average number of strong topics per document (threshold > 0.05)\n",
    "strong_topic_counts = (topic_matrix > 0.05).sum(axis=1)\n",
    "avg_strong_topics = np.mean(strong_topic_counts)\n",
    "print(f\"âœ… Average strong topics per doc (>0.05): {avg_strong_topics:.2f}\")\n",
    "\n",
    "# 2. Total topic usage across all documents\n",
    "topic_usage = topic_matrix.sum(axis=0)\n",
    "most_used_topics = topic_usage.argsort()[::-1]\n",
    "sorted_usage = topic_usage[most_used_topics]\n",
    "\n",
    "# 3. Bar plot: Total usage per topic\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(sorted_usage)), sorted_usage)\n",
    "plt.xlabel(\"Topic Index (sorted)\")\n",
    "plt.ylabel(\"Total Usage Across Documents\")\n",
    "plt.title(\"Topic Usage Distribution (LDA K=60)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Summary printout\n",
    "print(f\"Most used topic index: {most_used_topics[0]} (weight = {sorted_usage[0]:.2f})\")\n",
    "print(\n",
    "    f\"Least used topic index: {most_used_topics[-1]} (weight = {sorted_usage[-1]:.2f})\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
