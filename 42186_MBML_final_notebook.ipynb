{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e051b641",
   "metadata": {},
   "source": [
    "# 42186 Model-based machine learning\n",
    "- Matteo Piccagnoni s232713\n",
    "- Gabriel Lanaro s233541\n",
    "- Manuel Lovo s..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae297c",
   "metadata": {},
   "source": [
    "# Topic-aware SPAM message classification in Bayesian setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df129765",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90392517",
   "metadata": {},
   "source": [
    "\n",
    "Spam messages are annoying, and sometimes dangerous. Classifying them correctly is important, but we often ignore one key aspect: how confident are we in the predictions? \n",
    "\n",
    "In this project, we take a more thoughtful approach to spam detection by combining two powerful tools: **topic modeling** and **Bayesian inference**. \n",
    "\n",
    "First, we use **Latent Dirichlet Allocation (LDA)** to discover the hidden topics inside SMS messages, this gives us a better understanding of what the messages are about. \n",
    "Then, instead of using a standard classifier, we go full **Bayesian** with a **logistic regression model** that doesn’t just make a prediction, it tells us how uncertain that prediction is. \n",
    "\n",
    "Everything is built using Pyro, a probabilistic programming library, which makes it easy to define the model and run inference using both SVI and MCMC. \n",
    "\n",
    "This notebook walks through the whole process step-by-step: from data cleaning to topic discovery to classification and uncertainty analysis. By the end, we’ll not only have a working spam filter: we will have one that knows when it’s unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ecd096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa10898",
   "metadata": {},
   "source": [
    "## 2. Dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f8f785",
   "metadata": {},
   "source": [
    "### 2.1 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f5a0e",
   "metadata": {},
   "source": [
    "In this project, the **SMS Spam Collection Dataset** has been used. It is a publicly available corpus hosted by the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/228/sms+spam+collection) which contains 5,574 English SMS messages, each labeled as either \"ham\" (legitimate) or \"spam\" (unwanted/unsolicited).\n",
    "\n",
    "The messages were collected from a variety of sources:\n",
    "Legitimate (ham) messages were gathered from public forums, SMS chat services, and volunteer contributors.\n",
    "Spam messages were obtained from known spam databases and online archives of promotional SMS campaigns.\n",
    "\n",
    "Each row in the dataset consists of two fields:\n",
    "label: A string indicating whether the message is \"ham\" or \"spam\".\n",
    "message: The actual content of the SMS text, written in natural language (English).\n",
    "\n",
    "A few example rows:\n",
    "\n",
    "| label | message |\n",
    "|-------|---------|\n",
    "| ham   | Are you coming to the party later? |\n",
    "| spam  | You’ve won a £1000 cash prize! Text WIN to 80086 to claim now. |\n",
    "\n",
    "\n",
    "The dataset is realistic and includes a broad range of message types, from casual conversations full of slang and abbreviations to marketing promos and scams that mimic legitimate offers. This makes it ideal for studying both semantic patterns (via topic modeling) and predictive classification (spam vs. ham). The class distribution is slightly imbalanced, with around 13% spam and 87% ham, which reflects real-world conditions.\n",
    "\n",
    "Overall, this dataset offers a compact but rich playground for experimenting with natural language processing, especially when modeling uncertainty and interpretability, as we do in this Bayesian setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc0bc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sms = pd.read_csv(\n",
    "    \"SMSSpamCollection\", sep=\"\\t\", header=None, names=[\"label\", \"message\"]\n",
    ")\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c84033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sms.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8200a438",
   "metadata": {},
   "source": [
    "**NEED TO ADD SOME VISUALS TO MAKE IT COOLER AND TO HAVE A BETTER OVERVIEW OF THE DATASET from preprocessing.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8c751",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bb287",
   "metadata": {},
   "source": [
    "Before we can use the text messages in our models, we need to clean and prepare them. Raw SMS messages are often messy including typos, slang, special characters, and unnecessary words that can confuse a model. \n",
    "In this section, we’ll go through standard preprocessing steps like **lowercasing**, **removing stopwords**, **tokenizing**, and **stemming**. These transformations help reduce noise and bring the text into a more consistent format, which is especially important for tasks like topic modeling and classification. \n",
    "\n",
    "We’ll explain each step as we apply it in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37813ae",
   "metadata": {},
   "source": [
    "The following setup downloads the list of common English stopwords (like \"the\", \"is\", \"and\") from the NLTK library and initializes the stopword set and a Porter stemmer. These components will be used later to remove common words that carry little semantic meaning and to reduce words to their base form, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd5bc34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matteopiccagnoni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b61d67",
   "metadata": {},
   "source": [
    "The following function performs standard text preprocessing to prepare SMS messages for further analysis. It includes lowercasing, removal of URLs, numbers, and punctuation, followed by tokenization, stopword removal, and stemming. \n",
    "The result is a list of cleaned and normalized tokens for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "310f81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Function\n",
    "def clean_message(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = text.translate(\n",
    "        str.maketrans(\"\", \"\", string.punctuation)\n",
    "    )  # Remove punctuation\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9106d328",
   "metadata": {},
   "source": [
    "The cleaned tokenization function is applied to each message in the dataset using the apply method. A new column, \"tokens\", is created to store the resulting list of preprocessed tokens for each SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ceccfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entri, wkli, comp, win, fa, cup, final,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                      Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  U dun say so early hor... U c already then say...   \n",
       "4  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [go, jurong, point, crazi, avail, bugi, n, gre...  \n",
       "1                       [ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, wkli, comp, win, fa, cup, final,...  \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "4  [nah, dont, think, goe, usf, live, around, tho...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokens\"] = df[\"message\"].apply(clean_message)\n",
    "df[[\"message\", \"tokens\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f5c87",
   "metadata": {},
   "source": [
    "Above it is provides a quick preview of the structure and content of the DataFrame. At this stage, the output includes the raw message text, and the newly added tokens column, which contains the list of cleaned and preprocessed words extracted from each message. From the raw message to the tokenized version, the following elements were removed: URLs, numbers, punctuation, common stopwords, and words were reduced to their stemmed forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449ee90",
   "metadata": {},
   "source": [
    ">Please notice that in the tokenized output, some words appear in their stemmed form (for example, \"crazy\" becomes \"crazi\"). This is a result of the Porter stemming algorithm, which reduces words to their morphological root to group similar terms together. While the resulting stems are not always real words, they help the model treat related terms (e.g., \"crazy\", \"craziness\") as the same feature.\n",
    "It is also worth noting that abbreviations and slang (e.g., \"u\", \"ur\", \"msg\") were intentionally left unchanged. Although these do not follow standard grammar, they often carry important contextual or signals in SMS communication. Removing or expanding them could potentially obscure patterns that distinguish spam from ham in this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d936064",
   "metadata": {},
   "source": [
    "In this step, the list of tokens for each message is joined back into a single string to prepare the input for CountVectorizer, which requires text input in string format. The CountVectorizer transforms the preprocessed messages into a bag-of-words (BoW) matrix, where each row represents a message and each column corresponds to a word in the vocabulary, with entries indicating the word count per message.\n",
    "This BoW representation is a standard format for text modeling and is especially useful for Latent Dirichlet Allocation (LDA), which operates on document-word frequency data. The resulting matrix is converted into a PyTorch tensor to be compatible with Pyro, which is used for building and training the probabilistic topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3743495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input for LDA: torch.Size([5572, 7099])\n"
     ]
    }
   ],
   "source": [
    "texts_str = df[\"tokens\"].apply(lambda tokens: \" \".join(tokens)) # Join tokens back to strings (for CountVectorizer)\n",
    "\n",
    "# Create bag-of-words matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts_str)\n",
    "X_tensor = torch.tensor(X.toarray(), dtype=torch.float) # Convert to torch tensor\n",
    "\n",
    "print(\"Input for LDA:\", X_tensor.shape)  # should be (num_docs, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb8fcb5",
   "metadata": {},
   "source": [
    "The final print statement above shows the shape of the tensor, confirming that the data is now structured as (number of documents, vocabulary size), the expected input format for Pyro’s LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936c763",
   "metadata": {},
   "source": [
    "The example below illustrates the structure of the calculated data, providing a clearer understanding and overview of its format and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c18be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample tokens: ['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat']\n",
      "Sample BoW row: [212, 404, 790, 792, 1063, 1280, 2363, 2416, 2453, 3126, 3265, 4621, 6689, 6899]\n",
      "Word counts: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Words in BoW row: ['amor', 'avail', 'buffet', 'bugi', 'cine', 'crazi', 'go', 'got', 'great', 'jurong', 'la', 'point', 'wat', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Print first cleaned message as tokens\n",
    "print(\"\\nSample tokens:\", df[\"tokens\"].iloc[0])\n",
    "\n",
    "# Print the BoW vector for the first message (as vector index → count)\n",
    "print(\"Sample BoW row:\", X_tensor[0].nonzero(as_tuple=True)[0].tolist())\n",
    "print(\"Word counts:\", X_tensor[0][X_tensor[0] > 0].tolist())\n",
    "\n",
    "# Print the actual words from the vectorizer\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"Words in BoW row:\", [vocab[i] for i in X_tensor[0].nonzero(as_tuple=True)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe84b8c",
   "metadata": {},
   "source": [
    "This code saves the DataFrame with tokens, the fitted vectorizer, and the bag-of-words matrix. These components will be reused in the LDA model to ensure consistency and reproducibility in the topic modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DataFrame with tokens\n",
    "df.to_csv(\"lda_df.csv\", index=False)\n",
    "\n",
    "# save the vectorizer\n",
    "with open(\"lda_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# save bow matrix as numpy array\n",
    "X_array = X.toarray().astype(np.float32)\n",
    "np.savez_compressed(\"BoW_X_Array.npz\", X_array)\n",
    "\n",
    "print(\"File salvati:\")\n",
    "print(\"- lda_df.csv\")\n",
    "print(\"- lda_vectorizer.pkl\")\n",
    "print(\"- BoW_X_Array.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac61593",
   "metadata": {},
   "source": [
    "## 3. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75630d34",
   "metadata": {},
   "source": [
    "## 4. Bayesian Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21347d",
   "metadata": {},
   "source": [
    "## 5. Conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078e0a1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
